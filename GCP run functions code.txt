# cloud run function1 


import json
import functions_framework
from google.cloud import pubsub_v1
import pymysql
import os

PROJECT_ID = "decent-tracer-477412-g3"

# Pub/Sub topics
TOPIC_METADATA = "file-metadata"        # CSV, Parquet, JSON
TOPIC_OTHER = "uspubsub"                # All other file types

# ---------- CLOUD SQL CONFIG ----------
DB_USER = "ashok"                     # OR whichever user you created
DB_PASSWORD = "Ashok@1995"     # Replace
DB_NAME = "metadata_db"              # Replace with your DB name
INSTANCE_CONNECTION_NAME = "decent-tracer-477412-g3:asia-south1:ak-sql-instance"

def insert_mysql(bucket, file_name, size, content_type):
    """Insert metadata into Cloud SQL MySQL"""
    
    try:
        conn = pymysql.connect(
            user=DB_USER,
            password=DB_PASSWORD,
            unix_socket=f"/cloudsql/{INSTANCE_CONNECTION_NAME}",
            db=DB_NAME,
            cursorclass=pymysql.cursors.DictCursor
        )

        with conn.cursor() as cursor:
            insert_query = """
                INSERT INTO file_metadata (bucket, file_name, size, content_type)
                VALUES (%s, %s, %s, %s);
            """
            cursor.execute(insert_query, (bucket, file_name, size, content_type))
            conn.commit()

        conn.close()
        print("Inserted into MySQL successfully!")

    except Exception as e:
        print(f"MySQL Insert Error: {e}")


@functions_framework.cloud_event
def gcs_to_pubsub(event):
    """Triggered when a Cloud Storage file is created."""

    data = event.data
    bucket_name = data["bucket"]
    file_name = data["name"]
    size = data.get("size")
    content_type = data.get("contentType")

    # Build metadata payload
    message_data = {
        "bucket": bucket_name,
        "file_name": file_name,
        "size": size,
        "content_type": content_type,
    }

    print(f"Metadata extracted: {message_data}")

    # Avoid recursion for files written by Cloud Run
    if file_name.startswith("processing/") or file_name.startswith("errors/"):
        print(f"Ignored internal file: {file_name}")
        return "IGNORED"

    # ---- FILE TYPE ROUTING LOGIC ----
    file_ext = file_name.lower().split(".")[-1] if "." in file_name else ""
    structured_formats = {"csv", "parquet", "json"}

    if file_ext in structured_formats:
        target_topic = TOPIC_METADATA
        print(f"Structured file detected ‚Üí sending to {target_topic}")
    else:
        target_topic = TOPIC_OTHER
        print(f"Unstructured file detected ‚Üí sending to {target_topic}")

    # -------- PUBLISH TO PUB/SUB --------
    publisher = pubsub_v1.PublisherClient()
    topic_path = publisher.topic_path(PROJECT_ID, target_topic)

    publisher.publish(
        topic_path,
        json.dumps(message_data).encode("utf-8")
    )

    print(f"Message sent to Pub/Sub topic: {target_topic}")

    # -------- INSERT INTO MYSQL --------
    insert_mysql(bucket_name, file_name, size, content_type)

    return "OK"



# cloud run function 2

import base64
import json
import pandas as pd
import io
from google.cloud import storage
import functions_framework

@functions_framework.http
def process_metadata(request):
    envelope = request.get_json(silent=True)
    if not envelope or "message" not in envelope:
        return ("Bad Request", 400)

    try:
        pubsub_message = envelope["message"]
        metadata = json.loads(base64.b64decode(pubsub_message["data"]).decode("utf-8"))

        print("Received metadata:", metadata)

        # üî• FORCE DLQ TEST
        # REMOVE AFTER TESTING
        # raise Exception("Force DLQ test working")

        bucket_name = metadata["bucket"]
        file_name = metadata["file_name"]

        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(file_name)

        csv_bytes = blob.download_as_bytes()

        # CSV decode fallback
        try:
            df = pd.read_csv(io.BytesIO(csv_bytes), encoding="utf-8")
        except UnicodeDecodeError:
            df = pd.read_csv(io.BytesIO(csv_bytes), encoding="latin1")

        # Convert to parquet
        parquet_bytes = df.to_parquet(index=False)
        output_path = f"processing/{file_name.replace('.csv', '.parquet')}"
        
        bucket.blob(output_path).upload_from_string(parquet_bytes)

        return ("Success", 200)

    except Exception as e:
        print("ERROR sending message to DLQ:", e)
        return (str(e), 500)     # ‚Üê REQUIRED FOR DLQ



# cloud run function 3

import base64
import json
from google.cloud import storage
import functions_framework

@functions_framework.http
def handle_other_files(request):
    """Triggered by Pub/Sub2 for all non-CSV files"""
    
    envelope = request.get_json(silent=True)
    if not envelope or "message" not in envelope:
        return ("No Pub/Sub message received", 400)

    # pubsub_message = envelope["message"]

    metadata = json.loads(base64.b64decode(pubsub_message["data"]).decode("utf-8"))
    bucket_name = metadata["bucket"]
    file_name = metadata["file_name"]

    print(f"Handling other file type: {file_name}")
    
    if file_name.startswith("processing/"):
        print("Skipping processing folder file, no re-trigger.")
        return "OK"
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_name)

    # Destination path
    output_path = f"processing/{file_name}"

    print(f"Copying file to: {output_path}")

    new_blob = bucket.copy_blob(blob, bucket, output_path)

    print("File stored successfully!")
    return ("Success", 200)
